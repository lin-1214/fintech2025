{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtrHhbr7fRs5"
      },
      "source": [
        "# EE5183 Financial Technology - GNN Workshop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVZm-V1kfRs8"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EXW1dXy6ks2n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (2.6.0)\n",
            "Requirement already satisfied: torchvision in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (0.12.0)\n",
            "Requirement already satisfied: torchaudio in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (0.11.0)\n",
            "Requirement already satisfied: filelock in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from torch) (2025.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from torchvision) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/lin1214/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (2024.8.30)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
            "2.6.0\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'replace'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcu\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m version\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m CUDA_version \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda\n\u001b[0;32m---> 20\u001b[0m CUDA \u001b[38;5;241m=\u001b[39m \u001b[43mformat_cuda_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCUDA_version\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install torch-scatter -f https://pytorch-geometric.com/whl/torch-\u001b[39m\u001b[38;5;132;01m{TORCH}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{CUDA}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install torch-sparse -f https://pytorch-geometric.com/whl/torch-\u001b[39m\u001b[38;5;132;01m{TORCH}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{CUDA}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[6], line 16\u001b[0m, in \u001b[0;36mformat_cuda_version\u001b[0;34m(version)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_cuda_version\u001b[39m(version):\n\u001b[0;32m---> 16\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcu\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mversion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'"
          ]
        }
      ],
      "source": [
        "# Install required packages.\n",
        "!pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "\treturn version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "\treturn 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install pyg-lib -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UetU0ZAxRXVU"
      },
      "outputs": [],
      "source": [
        "# ----- Standard Libraries -----\n",
        "from typing import Optional, Tuple, Dict\n",
        "\n",
        "# ----- Third-party Packages -----\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----- PyTorch -----\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ----- PyTorch Geometric -----\n",
        "import torch_geometric\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nftm3xNSfRs-"
      },
      "source": [
        "## Helper Functions for Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVFmGuCOR--K"
      },
      "outputs": [],
      "source": [
        "# Helper function for visualization.\n",
        "def visualize_graph(G: nx.Graph, color: torch.Tensor) -> None:\n",
        "\t\"\"\"\n",
        "\tVisualizes a graph using NetworkX and Matplotlib.\n",
        "\n",
        "\tArgs:\n",
        "\t\tG (nx.Graph): The graph to visualize.\n",
        "\t\tcolor (torch.Tensor): A tensor containing node colors.\n",
        "\n",
        "\tReturns:\n",
        "\t\tNone\n",
        "\t\"\"\"\n",
        "\tplt.figure(figsize=(7, 7))\n",
        "\tplt.xticks([])\n",
        "\tplt.yticks([])\n",
        "\tnx.draw_networkx(\n",
        "\t\tG, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
        "\t\tnode_color=color, cmap=\"Set2\")\n",
        "\tplt.show()\n",
        "\n",
        "\n",
        "def visualize_embedding(\n",
        "\t\th: torch.Tensor,\n",
        "\t\tcolor: torch.Tensor,\n",
        "\t\tepoch: Optional[int] = None,\n",
        "\t\tloss: Optional[torch.Tensor] = None) -> None:\n",
        "\t\"\"\"\n",
        "\tVisualizes embeddings in a 2D space using Matplotlib.\n",
        "\n",
        "\tArgs:\n",
        "\t\th (torch.Tensor): The embedding tensor of shape (num_nodes, 2).\n",
        "\t\tcolor (torch.Tensor): A tensor containing node colors.\n",
        "\t\tepoch (Optional[int]): The current epoch number (default: None).\n",
        "\t\tloss (Optional[torch.Tensor]): The loss value (default: None).\n",
        "\n",
        "\tReturns:\n",
        "\t\tNone\n",
        "\t\"\"\"\n",
        "\tplt.figure(figsize=(7, 7))\n",
        "\tplt.xticks([])\n",
        "\tplt.yticks([])\n",
        "\th = h.detach().cpu().numpy()\n",
        "\tplt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
        "\tif epoch is not None and loss is not None:\n",
        "\t\tplt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
        "\tplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGKPF5w0p5jG"
      },
      "source": [
        "## Section 1 - Core Concepts of PyTorch Geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZQ-ntJcp-9h"
      },
      "source": [
        "### 1. Data Representation\n",
        "\n",
        "We show a simple example of an unweighted and undirected graph with three nodes and four edges. Each node contains exactly one feature:\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXPip1jCqEPJ"
      },
      "outputs": [],
      "source": [
        "# Define an edge list\n",
        "edge_index = torch.tensor([\n",
        "\t[0, 1, 1 ,2],   # source nodes\n",
        "\t[1, 0, 2, 1]], # target nodes\n",
        "\tdtype=torch.long)\n",
        "\n",
        "# Define node features (e.g., node labels or embeddings)\n",
        "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
        "\n",
        "# Create a Data object\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MK8-ZT6rpYQ"
      },
      "source": [
        "### 2. Datasets in PyG\n",
        "\n",
        "Following [Kipf et al. (2017)](https://arxiv.org/abs/1609.02907), let's dive into the world of GNNs by looking at a simple graph-structured example, the well-known [**Zachary's karate club network**](https://en.wikipedia.org/wiki/Zachary%27s_karate_club). This graph describes a social network of 34 members of a karate club and documents links between members who interacted outside the club. Here, we are interested in detecting communities that arise from the member's interaction.\n",
        "\n",
        "PyTorch Geometric provides an easy access to this dataset via the [`torch_geometric.datasets`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets) subpackage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-LoeVGqIYAp"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import KarateClub\n",
        "\n",
        "# Load the dataset\n",
        "dataset = KarateClub()\n",
        "\n",
        "# Inspect the dataset\n",
        "print(f\"Dataset: {dataset}\")\n",
        "print(\"======================\")\n",
        "print(f\"Number of graphs: {len(dataset)}\")\n",
        "print(f\"Node features: {dataset[0].num_node_features}\")\n",
        "print(f\"Classes: {dataset.num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6LvJVd0V-Fn"
      },
      "source": [
        "After initializing the [`KarateClub`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.KarateClub) dataset, we first can inspect some of its properties.\n",
        "For example, we can see that this dataset holds exactly **one graph**, and that each node in this dataset is assigned a **34-dimensional feature vector** (which uniquely describes the members of the karate club).\n",
        "Furthermore, the graph holds exactly **4 classes**, which represent the community each node belongs to.\n",
        "\n",
        "Let's now look at the underlying graph in more detail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivjz26D_WCOf"
      },
      "outputs": [],
      "source": [
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print(data)\n",
        "print('==============================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UvlockIWTQn"
      },
      "source": [
        "Each graph in PyTorch Geometric is represented by a single [`Data`](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data) object, which holds all the information to describe its graph representation.\n",
        "We can print the data object anytime via `print(data)` to receive a short summary about its attributes and their shapes:\n",
        "```\n",
        "Data(edge_index=[2, 156], x=[34, 34], y=[34], train_mask=[34])\n",
        "```\n",
        "We can see that this `data` object holds 4 attributes:\n",
        "(1) The `edge_index` property holds the information about the **graph connectivity**, *i.e.*, a tuple of source and destination node indices for each edge.\n",
        "PyG further refers to (2) **node features** as `x` (each of the 34 nodes is assigned a 34-dim feature vector), and to (3) **node labels** as `y` (each node is assigned to exactly one class).\n",
        "(4) There also exists an additional attribute called `train_mask`, which describes for which nodes we already know their community assigments.\n",
        "In total, we are only aware of the ground-truth labels of 4 nodes (one for each community), and the task is to infer the community assignment for the remaining nodes.\n",
        "\n",
        "The `data` object also provides some **utility functions** to infer some basic properties of the underlying graph.\n",
        "For example, we can easily infer whether there exists isolated nodes in the graph (*i.e.* there exists no edge to any node), whether the graph contains self-loops (*i.e.*, $(v, v) \\in \\mathcal{E}$), or whether the graph is undirected (*i.e.*, for each edge $(v, w) \\in \\mathcal{E}$ there also exists the edge $(w, v) \\in \\mathcal{E}$).\n",
        "\n",
        "Let us now inspect the `edge_index` property in more detail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EijZ5fmjWdmP"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "edge_index = data.edge_index\n",
        "print(edge_index.t())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZymeRSwgWfam"
      },
      "source": [
        "By printing `edge_index`, we can understand how PyG represents graph connectivity internally.\n",
        "We can see that for each edge, `edge_index` holds a tuple of two node indices, where the first value describes the node index of the source node and the second value describes the node index of the destination node of an edge.\n",
        "\n",
        "This representation is known as the **COO format (coordinate format)** commonly used for representing sparse matrices.\n",
        "Instead of holding the adjacency information in a dense representation $\\mathbf{A} \\in \\{ 0, 1 \\}^{|\\mathcal{V}| \\times |\\mathcal{V}|}$, PyG represents graphs sparsely, which refers to only holding the coordinates/values for which entries in $\\mathbf{A}$ are non-zero.\n",
        "\n",
        "Importantly, PyG does not distinguish between directed and undirected graphs, and treats undirected graphs as a special case of directed graphs in which reverse edges exist for every entry in `edge_index`.\n",
        "\n",
        "We can further visualize the graph by converting it to the `networkx` library format, which implements, in addition to graph manipulation functionalities, powerful tools for visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wINxGaiGXa0v"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "visualize_graph(G, color=data.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnY2o1zdfRtC"
      },
      "source": [
        "#### **Activity 1:** Try another dataset (except Cora) and report the statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecQcgWRXXiVX"
      },
      "source": [
        "### 3. Implementing Graph Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yr4Gei9Y7ag"
      },
      "source": [
        "#### 3.1 Dataset\n",
        "\n",
        "Here, we are given the ground-truth labels of only a small subset of nodes, and want to infer the labels for all the remaining nodes (*transductive learning*).\n",
        "\n",
        "To demonstrate, we make use of the `Cora` dataset, which is a **citation network** where nodes represent documents.\n",
        "Each node is described by a 1433-dimensional bag-of-words feature vector.\n",
        "Two documents are connected if there exists a citation link between them.\n",
        "The task is to infer the category of each document (7 in total).\n",
        "\n",
        "This dataset was first introduced by [Yang et al. (2016)](https://arxiv.org/abs/1603.08861) as one of the datasets of the `Planetoid` benchmark suite.\n",
        "We again can make use [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric) for an easy access to this dataset via [`torch_geometric.datasets.Planetoid`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZuZIBDSZHvu"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "dataset = Planetoid(\n",
        "\troot='./data/Planetoid',\n",
        "\tname='Cora',\n",
        "\ttransform=NormalizeFeatures())\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('===========================================================================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLzeuxHOZX2I"
      },
      "source": [
        "#### Bulding a Graph Neural Network\n",
        "\n",
        "Let's start with something simple yet effective: a Graph Convolutional Network (GCN). To recap, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)) is defined as\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{W}^{(\\ell + 1)}$ denotes a trainable weight matrix of shape `[num_output_features, num_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiF-5-SmZ38x"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "\t\"\"\"\n",
        "\tA Graph Convolutional Network (GCN) implementation using PyTorch Geometric.\n",
        "\n",
        "\tAttributes:\n",
        "\t\tconv1 (GCNConv): The first graph convolutional layer.\n",
        "\t\tconv2 (GCNConv): The second graph convolutional layer.\n",
        "\n",
        "\tMethods:\n",
        "\t\tforward(x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "\t\t\tDefines the forward pass of the GCN model.\n",
        "\t\"\"\"\n",
        "\t# TODO: Modify the architecture to achieve a better result.\n",
        "\tdef __init__(\n",
        "\t\t\tself,\n",
        "\t\t\tinput_channels: int,\n",
        "\t\t\thidden_channels: int,\n",
        "\t\t\toutput_channels: int) -> None:\n",
        "\t\t\"\"\"\n",
        "\t\tInitializes the GCN model.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\tinput_channels (int): Number of input features per node.\n",
        "\t\t\thidden_channels (int): Number of hidden units in the first layer.\n",
        "\t\t\toutput_channels (int): Number of output features per node.\n",
        "\t\t\"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\t\ttorch.manual_seed(1234567)\n",
        "\t\tself.conv1 = GCNConv(input_channels, hidden_channels)\n",
        "\t\tself.conv2 = GCNConv(hidden_channels, output_channels)\n",
        "\n",
        "\tdef forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "\t\t\"\"\"\n",
        "\t\tDefines the forward pass of the GCN model.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\tx (torch.Tensor): Node feature matrix of shape [num_nodes, num_features].\n",
        "\t\t\tedge_index (torch.Tensor): Graph connectivity in COO format with shape [2, num_edges].\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\ttorch.Tensor: Output node embeddings of shape [num_nodes, output_channels].\n",
        "\t\t\"\"\"\n",
        "\t\tx = self.conv1(x, edge_index)\n",
        "\t\tx = x.relu()\n",
        "\t\tx = F.dropout(x, p=0.5, training=self.training)\n",
        "\t\tx = self.conv2(x, edge_index)\n",
        "\t\treturn x\n",
        "\n",
        "model = GCN(\n",
        "\tinput_channels=dataset.num_features,\n",
        "\thidden_channels=16,\n",
        "\toutput_channels=dataset.num_classes)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mslDY5Uaasw2"
      },
      "source": [
        "#### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_fpXpHOa7au"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "# ----- Initialisation ----- #\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using {device}')\n",
        "\n",
        "# TODO: Change the hyperparameters to see if the accuracy increases\n",
        "model = GCN(\n",
        "\tinput_channels=dataset.num_features,\n",
        "\thidden_channels=16,\n",
        "\toutput_channels=dataset.num_classes)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "data = data.to(device)  # Move data to the same device as the model.\n",
        "\n",
        "\n",
        "# Lists to store training and validation losses and accuracies\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# ----- Training Function ----- #\n",
        "def train() -> Tuple[torch.Tensor, float]:\n",
        "\t\"\"\"\n",
        "\tTrains the model for one epoch.\n",
        "\n",
        "\tReturns:\n",
        "\t\tTuple[torch.Tensor, float]: The training loss and accuracy.\n",
        "\t\"\"\"\n",
        "\tmodel.train()\n",
        "\toptimizer.zero_grad()  # Clear gradients.\n",
        "\tout = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "\tloss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "\tloss.backward()  # Derive gradients.\n",
        "\toptimizer.step()  # Update parameters based on gradients.\n",
        "\tacc = test(data.train_mask)  # Compute training accuracy.\n",
        "\treturn loss, acc\n",
        "\n",
        "# ----- Validation Function ----- #\n",
        "def validate() -> Tuple[torch.Tensor, float]:\n",
        "\t\"\"\"\n",
        "\tValidates the model.\n",
        "\n",
        "\tReturns:\n",
        "\t\tTuple[torch.Tensor, float]: The validation loss and accuracy.\n",
        "\t\"\"\"\n",
        "\tmodel.eval()\n",
        "\twith torch.no_grad():\n",
        "\t\tout = model(data.x, data.edge_index)\n",
        "\t\tval_loss = criterion(out[data.val_mask], data.y[data.val_mask])  # Compute validation loss.\n",
        "\t\tacc = test(data.val_mask)  # Compute validation accuracy.\n",
        "\treturn val_loss, acc\n",
        "\n",
        "# ----- Testing Function ----- #\n",
        "def test(mask: torch.Tensor) -> float:\n",
        "\t\"\"\"\n",
        "\tTests the model on a given mask.\n",
        "\n",
        "\tArgs:\n",
        "\t\tmask (torch.Tensor): A boolean mask indicating the nodes to test on.\n",
        "\n",
        "\tReturns:\n",
        "\t\tfloat: The accuracy of the model on the given mask.\n",
        "\t\"\"\"\n",
        "\tmodel.eval()\n",
        "\tout = model(data.x, data.edge_index)\n",
        "\tpred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "\tcorrect = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
        "\tacc = int(correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
        "\treturn acc\n",
        "\n",
        "# ----- Training Loop ----- #\n",
        "for epoch in range(1, 101):\n",
        "\ttrain_loss, train_acc = train()\n",
        "\tval_loss, val_acc = validate()\n",
        "\ttrain_losses.append(train_loss.item())\n",
        "\tval_losses.append(val_loss.item())\n",
        "\ttrain_accuracies.append(train_acc)\n",
        "\tval_accuracies.append(val_acc)\n",
        "\tprint(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "# ----- Visualising the Training Process ----- #\n",
        "# Plot training and validation losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, 101), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, 101), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracies\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, 101), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, 101), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracies')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0QLbdoSbCe4"
      },
      "source": [
        "After training the model, we can check its test accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZgIO-zCbFQV"
      },
      "outputs": [],
      "source": [
        "# ----- Evaluation on the Test Set ----- #\n",
        "test_acc = test(data.test_mask)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxFzLC6tfRtD"
      },
      "source": [
        "#### **Activity 2:** Try to achieve a higher test accuracy for the GCN and report the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAvJsINVdEC4"
      },
      "source": [
        "#### **Activity 3:** Change to GAT and report the test accuracy\n",
        "\n",
        "Try to use different GNN layers to see how model performance changes. What happens if you swap out all `GCNConv` instances with [`GATConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATConv) layers that make use of attention? Try to write a 2-layer `GAT` model that makes use of 8 attention heads in the first layer and 1 attention head in the second layer, uses a `dropout` ratio of `0.6` inside and outside each `GATConv` call, and uses a `hidden_channels` dimensions of `8` per head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi6OYiZrdXC4"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "\tdef __init__(self, hidden_channels, heads):\n",
        "\t\tsuper().__init__()\n",
        "\t\ttorch.manual_seed(1234567)\n",
        "\t\tself.conv1 = GATConv(...)  # TODO\n",
        "\t\tself.conv2 = GATConv(...)  # TODO\n",
        "\n",
        "\tdef forward(self, x, edge_index):\n",
        "\t\tx = F.dropout(x, p=0.6, training=self.training)\n",
        "\t\tx = self.conv1(x, edge_index)\n",
        "\t\tx = F.elu(x)\n",
        "\t\tx = F.dropout(x, p=0.6, training=self.training)\n",
        "\t\tx = self.conv2(x, edge_index)\n",
        "\t\treturn x\n",
        "\n",
        "model = GAT(hidden_channels=8, heads=8)\n",
        "print(model)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "\tmodel.train()\n",
        "\toptimizer.zero_grad()  # Clear gradients.\n",
        "\tout = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "\tloss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "\tloss.backward()  # Derive gradients.\n",
        "\toptimizer.step()  # Update parameters based on gradients.\n",
        "\treturn loss\n",
        "\n",
        "def test(mask):\n",
        "\tmodel.eval()\n",
        "\tout = model(data.x, data.edge_index)\n",
        "\tpred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "\tcorrect = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
        "\tacc = int(correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
        "\treturn acc\n",
        "\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "\tloss = train()\n",
        "\tval_acc = test(data.val_mask)\n",
        "\ttest_acc = test(data.test_mask)\n",
        "\tprint(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YkkMI2tfRtE"
      },
      "source": [
        "## Section 2: From Homogeneous to Heterogeneous GNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOwjzZKaVwbM"
      },
      "source": [
        "### 1. Dataset: DBLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ5KtrblWkfb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "from itertools import product\n",
        "from typing import Callable, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch_geometric.data import (\n",
        "    HeteroData,\n",
        "    InMemoryDataset,\n",
        "    download_url,\n",
        "    extract_zip,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V22hg3g0WaLD"
      },
      "outputs": [],
      "source": [
        "# ----- Define the Dataset Class ----- #\n",
        "class DBLP(InMemoryDataset):\n",
        "    r\"\"\"A subset of the DBLP computer science bibliography website, as\n",
        "    collected in the `\"MAGNN: Metapath Aggregated Graph Neural Network for\n",
        "    Heterogeneous Graph Embedding\" <https://arxiv.org/abs/2002.01680>`_ paper.\n",
        "    DBLP is a heterogeneous graph containing four types of entities - authors\n",
        "    (4,057 nodes), papers (14,328 nodes), terms (7,723 nodes), and conferences\n",
        "    (20 nodes).\n",
        "    The authors are divided into four research areas (database, data mining,\n",
        "    artificial intelligence, information retrieval).\n",
        "    Each author is described by a bag-of-words representation of their paper\n",
        "    keywords.\n",
        "\n",
        "    Args:\n",
        "        root (str): Root directory where the dataset should be saved.\n",
        "        transform (callable, optional): A function/transform that takes in an\n",
        "            :obj:`torch_geometric.data.HeteroData` object and returns a\n",
        "            transformed version. The data object will be transformed before\n",
        "            every access. (default: :obj:`None`)\n",
        "        pre_transform (callable, optional): A function/transform that takes in\n",
        "            an :obj:`torch_geometric.data.HeteroData` object and returns a\n",
        "            transformed version. The data object will be transformed before\n",
        "            being saved to disk. (default: :obj:`None`)\n",
        "        force_reload (bool, optional): Whether to re-process the dataset.\n",
        "            (default: :obj:`False`)\n",
        "    \"\"\"\n",
        "\n",
        "    url = 'https://www.dropbox.com/s/yh4grpeks87ugr2/DBLP_processed.zip?dl=1'\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        transform: Optional[Callable] = None,\n",
        "        pre_transform: Optional[Callable] = None,\n",
        "        force_reload: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(root, transform, pre_transform,\n",
        "                         force_reload=force_reload)\n",
        "        self.load(self.processed_paths[0], data_cls=HeteroData)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> List[str]:\n",
        "        return [\n",
        "            'adjM.npz', 'features_0.npz', 'features_1.npz', 'features_2.npy',\n",
        "            'labels.npy', 'node_types.npy', 'train_val_test_idx.npz'\n",
        "        ]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self) -> str:\n",
        "        return 'data.pt'\n",
        "\n",
        "    def download(self) -> None:\n",
        "        path = download_url(self.url, self.raw_dir)\n",
        "        extract_zip(path, self.raw_dir)\n",
        "        os.remove(path)\n",
        "\n",
        "    def process(self) -> None:\n",
        "        import scipy.sparse as sp\n",
        "\n",
        "        data = HeteroData()\n",
        "\n",
        "        node_types = ['author', 'paper', 'term', 'conference']\n",
        "        for i, node_type in enumerate(node_types[:2]):\n",
        "            x = sp.load_npz(osp.join(self.raw_dir, f'features_{i}.npz'))\n",
        "            data[node_type].x = torch.from_numpy(x.todense()).to(torch.float)\n",
        "\n",
        "        x = np.load(osp.join(self.raw_dir, 'features_2.npy'))\n",
        "        data['term'].x = torch.from_numpy(x).to(torch.float)\n",
        "\n",
        "        node_type_idx = np.load(osp.join(self.raw_dir, 'node_types.npy'))\n",
        "        node_type_idx = torch.from_numpy(node_type_idx).to(torch.long)\n",
        "        data['conference'].num_nodes = int((node_type_idx == 3).sum())\n",
        "\n",
        "        y = np.load(osp.join(self.raw_dir, 'labels.npy'))\n",
        "        data['author'].y = torch.from_numpy(y).to(torch.long)\n",
        "\n",
        "        split = np.load(osp.join(self.raw_dir, 'train_val_test_idx.npz'))\n",
        "        for name in ['train', 'val', 'test']:\n",
        "            idx = split[f'{name}_idx']\n",
        "            idx = torch.from_numpy(idx).to(torch.long)\n",
        "            mask = torch.zeros(data['author'].num_nodes, dtype=torch.bool)\n",
        "            mask[idx] = True\n",
        "            data['author'][f'{name}_mask'] = mask\n",
        "\n",
        "        s = {}\n",
        "        N_a = data['author'].num_nodes\n",
        "        N_p = data['paper'].num_nodes\n",
        "        N_t = data['term'].num_nodes\n",
        "        N_c = data['conference'].num_nodes\n",
        "        s['author'] = (0, N_a)\n",
        "        s['paper'] = (N_a, N_a + N_p)\n",
        "        s['term'] = (N_a + N_p, N_a + N_p + N_t)\n",
        "        s['conference'] = (N_a + N_p + N_t, N_a + N_p + N_t + N_c)\n",
        "\n",
        "        A = sp.load_npz(osp.join(self.raw_dir, 'adjM.npz'))\n",
        "        for src, dst in product(node_types, node_types):\n",
        "            A_sub = A[s[src][0]:s[src][1], s[dst][0]:s[dst][1]].tocoo()\n",
        "            if A_sub.nnz > 0:\n",
        "                row = torch.from_numpy(A_sub.row).to(torch.long)\n",
        "                col = torch.from_numpy(A_sub.col).to(torch.long)\n",
        "                data[src, dst].edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data = self.pre_transform(data)\n",
        "\n",
        "        self.save([data], self.processed_paths[0])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f'{self.__class__.__name__}()'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMQXoT53W7PU"
      },
      "outputs": [],
      "source": [
        "# ----- Download and Process the Dataset ----- #\n",
        "path = \"./data/dblp\"\n",
        "os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# Download dataset\n",
        "dataset = DBLP(\n",
        "    root=path,\n",
        "    transform=T.Compose([\n",
        "        T.Constant(node_types=\"conference\")]),\n",
        "    force_reload=True)\n",
        "data = dataset[0]\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDlDzprEX2zU"
      },
      "outputs": [],
      "source": [
        "# ----- Graph Statistics ----- #\n",
        "# Single node/edge data\n",
        "print(data[\"paper\"], \"\\n\", data[\"paper\", \"to\", \"author\"])\n",
        "\n",
        "# Access the meta-data\n",
        "node_types, edge_types = data.metadata()\n",
        "print(node_types, \"\\n\",  edge_types)\n",
        "\n",
        "# Additional helper analysis functions\n",
        "print(f\"Has isolated nodes: {data.has_isolated_nodes()}\")\n",
        "print(f\"Has self loops: {data.has_self_loops()}\")\n",
        "print(f\"Is undirected: {data.is_undirected()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5GwBM9Cm3cO"
      },
      "outputs": [],
      "source": [
        "print(f\"Training nodes: {data['author'].train_mask.sum()}\")\n",
        "print(f\"Validation nodes: {data['author'].val_mask.sum()}\")\n",
        "print(f\"Test nodes: {data['author'].test_mask.sum()}\")\n",
        "print(f\"Unique labels: {data['author'].y.unique()}\")\n",
        "num_classes = len(data[\"author\"].y.unique())\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdoVFwYmX8z0"
      },
      "outputs": [],
      "source": [
        "# ----- Convert to a homogeneous graph ----- #\n",
        "homogeneous_data = data.to_homogeneous()\n",
        "print(homogeneous_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TG7zH7FYBDr"
      },
      "source": [
        "### 2. Heterogeneous Graph Samplers\n",
        "\n",
        "PyG provides various functionalities for sampling heterogenenous graphs, i.e., in the standard `torch_geometric.loader.NeighborLoader` class or in dedicated heterogeneous graph samplers such as `torch_geometric.loader.HGTLoader`. This is especially useful for efficient representation learning on larger heterogeneous graphs, where processing the full number of neighbours is too computationally expensive.\n",
        "\n",
        "Overall, all heterogeneous graph loaders will prodcue a `HeteroData` object as output, holding a subset of the original data, and mainly differ in the way their sampling procedures works. As such, only minimal code changes are required to convert the training procedure from full-batch training to mini-batch training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDkTh_uBYNus"
      },
      "outputs": [],
      "source": [
        "# ----- Graph Neighbour Sampling ----- #\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Send node features/labels to GPU for faster access during sampling\n",
        "data = data.to(device, \"x\", \"y\")\n",
        "\n",
        "# Extract the training/validation/test nodes\n",
        "train_input_nodes = (\"author\", data[\"author\"].train_mask)\n",
        "val_input_nodes = (\"author\", data[\"author\"].val_mask)\n",
        "test_input_nodes = (\"author\", data[\"author\"].test_mask)\n",
        "\n",
        "kwargs = {'batch_size': 1024, 'num_workers': 2, 'persistent_workers': True}\n",
        "\n",
        "# DataLoader.\n",
        "# TODO: Small graph, change the number of neighbours\n",
        "train_loader = torch_geometric.loader.NeighborLoader(\n",
        "    data=data,\n",
        "    # Sample 15 neighbours for each node and each edge type for 5 iterations\n",
        "    num_neighbors=[15] * 5,\n",
        "    shuffle=True,\n",
        "    input_nodes=train_input_nodes,\n",
        "    **kwargs)\n",
        "val_loader = torch_geometric.loader.NeighborLoader(\n",
        "    data=data,\n",
        "    num_neighbors=[15] * 5,\n",
        "    shuffle=False,\n",
        "    input_nodes=val_input_nodes,\n",
        "    **kwargs)\n",
        "test_loader = torch_geometric.loader.NeighborLoader(\n",
        "    data=data,\n",
        "    num_neighbors=[15] * 5,\n",
        "    shuffle=False,\n",
        "    input_nodes=test_input_nodes,\n",
        "    **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnswktGEYSC0"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOfYV6nJYVgr"
      },
      "source": [
        "### 3. Creating Heterogeneous GNNs\n",
        "\n",
        "Standard Message Passing GNNs (MP-GNNs) cannot trivially be applied to heterogeneous graph data, as node and edge features from different types cannot be processed by the same functions due to differences in feature type. A natural way to circumvent this is to implement message and update functions individually for each eage type. During runtime, the MP-GNN algorithm would need to iterate over edge type dictionaries during node updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fg-mJ-wYbBE"
      },
      "source": [
        "#### 3.1 Automatically Converting GNN Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18IMoNfiYg4J"
      },
      "outputs": [],
      "source": [
        "# ----- GAT with Skip-connections ----- #\n",
        "from torch_geometric.nn import GATConv, Linear, to_hetero\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "\t\"\"\"\n",
        "\tA Graph Attention Network (GAT) implementation with skip-connections.\n",
        "\n",
        "\tAttributes:\n",
        "\t\tconv1 (GATConv): The first graph attention layer.\n",
        "\t\tlin1 (Linear): Linear layer for skip-connection in the first layer.\n",
        "\t\tconv2 (GATConv): The second graph attention layer.\n",
        "\t\tlin2 (Linear): Linear layer for skip-connection in the second layer.\n",
        "\n",
        "\tMethods:\n",
        "\t\tforward(x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "\t\t\tDefines the forward pass of the GAT model.\n",
        "\t\"\"\"\n",
        "\t# TODO: Modify the architecture to achieve a better result\n",
        "\tdef __init__(self, hidden_channels: int, out_channels: int) -> None:\n",
        "\t\t\"\"\"\n",
        "\t\tInitializes the GAT model.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\thidden_channels (int): Number of hidden units in the first layer.\n",
        "\t\t\tout_channels (int): Number of output features per node.\n",
        "\t\t\"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.conv1 = GATConv(\n",
        "\t\t\tin_channels=(-1, -1),\n",
        "\t\t\tout_channels=hidden_channels,\n",
        "\t\t\theads=1,  # default value\n",
        "\t\t\tadd_self_loops=False)\n",
        "\t\tself.lin1 = Linear(\n",
        "\t\t\tin_channels=-1,\n",
        "\t\t\tout_channels=hidden_channels)\n",
        "\t\tself.conv2 = GATConv(\n",
        "\t\t\tin_channels=(-1, -1),\n",
        "\t\t\tout_channels=out_channels,\n",
        "\t\t\theads=1,  # default value\n",
        "\t\t\tadd_self_loops=False)\n",
        "\t\tself.lin2 = Linear(\n",
        "\t\t\tin_channels=-1,\n",
        "\t\t\tout_channels=out_channels)\n",
        "\n",
        "\tdef forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "\t\t\"\"\"\n",
        "\t\tDefines the forward pass of the GAT model.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\tx (torch.Tensor): Node feature matrix of shape [num_nodes, num_features].\n",
        "\t\t\tedge_index (torch.Tensor): Graph connectivity in COO format with shape [2, num_edges].\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\ttorch.Tensor: Output node embeddings of shape [num_nodes, out_channels].\n",
        "\t\t\"\"\"\n",
        "\t\tx = self.conv1(x, edge_index) + self.lin1(x)\n",
        "\t\tx = x.relu()\n",
        "\t\tx = self.conv2(x, edge_index) + self.lin2(x)\n",
        "\t\treturn x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5JGIEHVae0U"
      },
      "outputs": [],
      "source": [
        "# ----- Initialisation ----- #\n",
        "# num_classes = len(data[\"author\"].y.unique())\n",
        "# print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "gat_model = GAT(hidden_channels=64, out_channels=num_classes)\n",
        "gat_model = to_hetero(gat_model, data.metadata(), aggr=\"sum\")\n",
        "# print(gat_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFr0b7tXalz8"
      },
      "source": [
        "The process takes an existing GNN model and duplicates the message passing fuctions to work on each edge type individually.\n",
        "\n",
        "As the result, the model now expects dictionaries with node and edge types as keys as input arguments, rather than single tensors utilised in homogeneous graphs.\n",
        "\n",
        "<b><u>Note</u>:</b> Since the number of input features and thus the size of tensors varies between different types, PyG can make use of **lazy intialisation** to initialise parameters in heterogeneous GNNs (as denoted by `-1` as the `in_channels` argument). This allows us to avoide calculating and keeping track of all tensor sizes of the computation graph. Lazy intialisation is supoported for all existing PyG operators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA0Pix7ddpWd"
      },
      "outputs": [],
      "source": [
        "# ----- Training Function ----- #\n",
        "def train(\n",
        "\t\tmodel: torch.nn.Module,\n",
        "\t\tloader: torch_geometric.loader.NeighborLoader,\n",
        "\t\toptimizer: torch.optim.Optimizer,\n",
        "\t\tdevice: torch.device) -> float:\n",
        "\t\"\"\"\n",
        "\tTrains the model for one epoch.\n",
        "\n",
        "\tArgs:\n",
        "\t\tmodel (torch.nn.Module): The model to train.\n",
        "\t\tloader (torch_geometric.loader.NeighborLoader): The data loader for training batches.\n",
        "\t\toptimizer (torch.optim.Optimizer): The optimizer used for training.\n",
        "\t\tdevice (torch.device): The device to run the training on (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "\tReturns:\n",
        "\t\tfloat: The average training loss for the epoch.\n",
        "\t\"\"\"\n",
        "\tmodel.train()\n",
        "\n",
        "\ttotal_examples = total_loss = 0\n",
        "\n",
        "\tfor batch in tqdm(loader):\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tbatch = batch.to(device, \"edge_index\")\n",
        "\t\tbatch_size = batch[\"author\"].batch_size\n",
        "\t\t# Pay attention to the output of the model\n",
        "\t\tout = model(batch.x_dict, batch.edge_index_dict)[\"author\"][:batch_size]\n",
        "\t\tloss = F.cross_entropy(out, batch[\"author\"].y[:batch_size])\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\ttotal_examples += batch_size\n",
        "\t\ttotal_loss += float(loss) * batch_size\n",
        "\n",
        "\treturn total_loss / total_examples\n",
        "\n",
        "# ----- Evaluation Function ----- #\n",
        "@torch.no_grad()\n",
        "def test(\n",
        "\tmodel: torch.nn.Module,\n",
        "\tloader: torch_geometric.loader.NeighborLoader,\n",
        "\tdevice: torch.device) -> float:\n",
        "\t\"\"\"\n",
        "\tEvaluates the model on a given dataset.\n",
        "\n",
        "\tArgs:\n",
        "\t\tmodel (torch.nn.Module): The model to evaluate.\n",
        "\t\tloader (torch_geometric.loader.NeighborLoader): The data loader for evaluation batches.\n",
        "\t\tdevice (torch.device): The device to run the evaluation on (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "\tReturns:\n",
        "\t\tfloat: The accuracy of the model on the evaluation dataset.\n",
        "\t\"\"\"\n",
        "\tmodel.eval()\n",
        "\n",
        "\ttotal_examples = total_correct = 0\n",
        "\n",
        "\tfor batch in tqdm(loader):\n",
        "\t\tbatch = batch.to(device, \"edge_index\")\n",
        "\t\tbatch_size = batch[\"author\"].batch_size\n",
        "\t\tout = model(batch.x_dict, batch.edge_index_dict)[\"author\"][:batch_size]\n",
        "\t\tpred = out.argmax(dim=-1)\n",
        "\n",
        "\t\ttotal_examples += batch_size\n",
        "\t\ttotal_correct += int((pred == batch[\"author\"].y[:batch_size]).sum())\n",
        "\n",
        "\treturn total_correct / total_examples\n",
        "\n",
        "# ----- Function to Initialise Parameters ----- #\n",
        "@torch.no_grad()\n",
        "def init_params(\n",
        "\tmodel: torch.nn.Module,\n",
        "\tloader: torch_geometric.loader.NeighborLoader,\n",
        "\tdevice: torch.device) -> None:\n",
        "\t\"\"\"\n",
        "\tInitialises lazy parameters by forwarding a single batch to the model.\n",
        "\n",
        "\tArgs:\n",
        "\t\tmodel (torch.nn.Module): The model to initialise.\n",
        "\t\tloader (torch_geometric.loader.NeighborLoader): The data loader for a single batch.\n",
        "\t\tdevice (torch.device): The device to run the initialisation on (e.g., 'cuda' or 'cpu').\n",
        "\t\"\"\"\n",
        "\tbatch = next(iter(loader))\n",
        "\tbatch = batch.to(device, \"edge_index\")\n",
        "\tmodel(batch.x_dict, batch.edge_index_dict)\n",
        "\n",
        "# ----- Initialisation ----- #\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gat_model = gat_model.to(device)\n",
        "\n",
        "init_params(model=gat_model, loader=train_loader, device=device)\n",
        "optimizer = torch.optim.Adam(gat_model.parameters(), lr=0.01) #TODO: adjust LR\n",
        "\n",
        "# Print the test accuracy before training\n",
        "print(f\"Test: {test(gat_model, test_loader, device): 0.4f} \\n\")\n",
        "\n",
        "# ----- Training Loop ----- #\n",
        "for epoch in range(1, 51):\n",
        "    loss = train(gat_model, train_loader, optimizer, device)\n",
        "    val_acc = test(gat_model, val_loader, device)\n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}\")\n",
        "\n",
        "print(f\"\\n Test: {test(gat_model, test_loader, device): 0.4f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxPFuH4FerZM"
      },
      "source": [
        "#### 3.2 Using the Heterogeneous Convolution Wrapper\n",
        "\n",
        "The heterogeneous convolution wrapper `torch_geometric.nn.conv.HeteroConv` allows to define custom heterogeneous message and update functions to build arbitrary MP-GNNs for heterogeneous graphs from scratch. While the automatic converter `to_hetero()` uses the same operator for all edge types, the wrapper allows to define different operators for different edge types. Here, `HeteroConv` takes a dictionary of submodules as input, one for each edge type in the graph data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rZxOZbLfReV"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import HeteroConv, SAGEConv, GATConv, Linear\n",
        "\n",
        "class HeteroGNN(torch.nn.Module):\n",
        "\t\"\"\"\n",
        "\tA Heterogeneous Graph Neural Network (HeteroGNN) implementation using PyTorch Geometric.\n",
        "\n",
        "\tAttributes:\n",
        "\t\tconvs (torch.nn.ModuleList): A list of HeteroConv layers for message passing.\n",
        "\t\tlin (torch.nn.Linear): A linear layer for final node classification.\n",
        "\n",
        "\tMethods:\n",
        "\t\tforward(x_dict: Dict[str, torch.Tensor], edge_index_dict: Dict[Tuple[str, str, str], torch.Tensor]) -> torch.Tensor:\n",
        "\t\t\tDefines the forward pass of the HeteroGNN model.\n",
        "\t\"\"\"\n",
        "\tdef __init__(\n",
        "\t\t\tself,\n",
        "\t\t\thidden_channels: int,\n",
        "\t\t\tout_channels: int,\n",
        "\t\t\tnum_layers: int) -> None:\n",
        "\t\t\"\"\"\n",
        "\t\tInitializes the HeteroGNN model.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\thidden_channels (int): Number of hidden units in each layer.\n",
        "\t\t\tout_channels (int): Number of output features per node.\n",
        "\t\t\tnum_layers (int): Number of HeteroConv layers in the model.\n",
        "\t\t\"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tself.convs = torch.nn.ModuleList()\n",
        "\n",
        "\t\tfor _ in range(num_layers):\n",
        "\t\t\tconv = HeteroConv({\n",
        "\t\t\t\t('author', 'to', 'paper'): SAGEConv(\n",
        "\t\t\t\t\tin_channels=(-1, -1),\n",
        "\t\t\t\t\tout_channels=hidden_channels),\n",
        "\t\t\t\t('paper', 'to', 'term'): SAGEConv(\n",
        "\t\t\t\t\tin_channels=(-1, -1),\n",
        "\t\t\t\t\tout_channels=hidden_channels),\n",
        "\t\t\t\t('paper', 'to', 'conference'): SAGEConv(\n",
        "\t\t\t\t\tin_channels=(-1, -1),\n",
        "\t\t\t\t\tout_channels=hidden_channels),\n",
        "\t\t\t\t('paper', 'to', 'author'): GATConv(\n",
        "\t\t\t\t\tin_channels=(-1, -1),\n",
        "\t\t\t\t\tout_channels=hidden_channels,\n",
        "\t\t\t\t\tadd_self_loops=False),\n",
        "\t\t\t\t('paper', 'to', 'term'): GATConv(\n",
        "\t\t\t\t\tin_channels=(-1, -1),\n",
        "\t\t\t\t\tout_channels=hidden_channels,\n",
        "\t\t\t\t\tadd_self_loops=False),\n",
        "\t\t\t\t('paper', 'to', 'conference'): GATConv(\n",
        "\t\t\t\t\tin_channels=(-1, -1),\n",
        "\t\t\t\t\tout_channels=hidden_channels,\n",
        "\t\t\t\t\tadd_self_loops=False),\n",
        "\t\t\t}, aggr='sum')\n",
        "\t\t\tself.convs.append(conv)\n",
        "\n",
        "\t\tself.lin = Linear(\n",
        "\t\t\tin_channels=hidden_channels,\n",
        "\t\t\tout_channels=out_channels)\n",
        "\n",
        "\tdef forward(\n",
        "\t\t\tself,\n",
        "\t\t\tx_dict: Dict[str, torch.Tensor],\n",
        "\t\t\tedge_index_dict: Dict[Tuple[str, str, str], torch.Tensor]) -> torch.Tensor:\n",
        "\t\t\"\"\"\n",
        "\t\tDefines the forward pass of the HeteroGNN model.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\tx_dict (Dict[str, torch.Tensor]): A dictionary containing node feature matrices for each node type.\n",
        "\t\t\tedge_index_dict (Dict[Tuple[str, str, str], torch.Tensor]): A dictionary containing edge indices for each edge type.\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\ttorch.Tensor: Output node embeddings for the 'author' node type.\n",
        "\t\t\"\"\"\n",
        "\t\tfor conv in self.convs:\n",
        "\t\t\tx_dict = conv(x_dict, edge_index_dict)\n",
        "\t\t\tx_dict = {key: x.relu() for key, x in x_dict.items()}\n",
        "\n",
        "\t\treturn self.lin(x_dict['author'])\n",
        "\n",
        "wrapper_model = HeteroGNN(\n",
        "    hidden_channels=64,\n",
        "    out_channels=num_classes,\n",
        "    num_layers=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea5GrJzxgVkf"
      },
      "outputs": [],
      "source": [
        "# ----- Training Function ----- #\n",
        "def train(\n",
        "\t\tmodel: torch.nn.Module,\n",
        "\t\tloader: torch_geometric.loader.NeighborLoader,\n",
        "\t\toptimizer: torch.optim.Optimizer,\n",
        "\t\tdevice: torch.device) -> float:\n",
        "\t\"\"\"\n",
        "\tTrains the model for one epoch.\n",
        "\n",
        "\tArgs:\n",
        "\t\tmodel (torch.nn.Module): The model to train.\n",
        "\t\tloader (torch_geometric.loader.NeighborLoader): The data loader for training batches.\n",
        "\t\toptimizer (torch.optim.Optimizer): The optimizer used for training.\n",
        "\t\tdevice (torch.device): The device to run the training on (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "\tReturns:\n",
        "\t\tfloat: The average training loss for the epoch.\n",
        "\t\"\"\"\n",
        "\tmodel.train()\n",
        "\n",
        "\ttotal_examples = total_loss = 0\n",
        "\n",
        "\tfor batch in tqdm(loader):\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tbatch = batch.to(device, \"edge_index\")\n",
        "\t\tbatch_size = batch[\"author\"].batch_size\n",
        "\t\t# Note: Output different from the previous one\n",
        "\t\tout = model(batch.x_dict, batch.edge_index_dict)[:batch_size]\n",
        "\t\tloss = F.cross_entropy(out, batch[\"author\"].y[:batch_size])\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\ttotal_examples += batch_size\n",
        "\t\ttotal_loss += float(loss) * batch_size\n",
        "\n",
        "\treturn total_loss / total_examples\n",
        "\n",
        "# ----- Evaluation Function ----- #\n",
        "@torch.no_grad()\n",
        "def test(model: torch.nn.Module, loader: torch_geometric.loader.NeighborLoader, device: torch.device) -> float:\n",
        "\t\"\"\"\n",
        "\tEvaluates the model on a given dataset.\n",
        "\n",
        "\tArgs:\n",
        "\t\tmodel (torch.nn.Module): The model to evaluate.\n",
        "\t\tloader (torch_geometric.loader.NeighborLoader): The data loader for evaluation batches.\n",
        "\t\tdevice (torch.device): The device to run the evaluation on (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "\tReturns:\n",
        "\t\tfloat: The accuracy of the model on the evaluation dataset.\n",
        "\t\"\"\"\n",
        "\tmodel.eval()\n",
        "\n",
        "\ttotal_examples = total_correct = 0\n",
        "\n",
        "\tfor batch in tqdm(loader):\n",
        "\t\tbatch = batch.to(device, \"edge_index\")\n",
        "\t\tbatch_size = batch[\"author\"].y.size(0)\n",
        "\t\t# The output of the model is directly used\n",
        "\t\tout = model(batch.x_dict, batch.edge_index_dict)\n",
        "\t\tpred = out.argmax(dim=-1)\n",
        "\n",
        "\t\ttotal_examples += batch_size\n",
        "\t\ttotal_correct += int((pred == batch[\"author\"].y[:batch_size]).sum())\n",
        "\n",
        "\treturn total_correct / total_examples\n",
        "\n",
        "# ----- Initialisation ----- #\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wrapper_model = wrapper_model.to(device)\n",
        "\n",
        "init_params(model=wrapper_model, loader=train_loader, device=device)\n",
        "optimizer = torch.optim.Adam(wrapper_model.parameters(), lr=0.01)\n",
        "print(f\"Test: {test(wrapper_model, test_loader, device): 0.4f} \\n\")\n",
        "\n",
        "# ----- Training Loop ----- #\n",
        "for epoch in range(1, 51):\n",
        "    loss = train(wrapper_model, train_loader, optimizer, device)\n",
        "    val_acc = test(wrapper_model, val_loader, device)\n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}\")\n",
        "\n",
        "print(f\"\\n Test: {test(wrapper_model, test_loader, device): 0.4f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN7JhbV6hI-m"
      },
      "source": [
        "#### 3.3 Deploying Existing Heterogeneous Operators\n",
        "\n",
        "PyG provides operators (e.g., `torch_geometric.nn.conv.HGTConv`), which are specifically desgined for heterogeneous graphs. These operators can be directly used to build heterogeneous GNN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR-9Q-vYhmA9"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import HGTConv\n",
        "\n",
        "class HGT(torch.nn.Module):\n",
        "\t\"\"\"\n",
        "\tA Heterogeneous Graph Transformer (HGT) implementation using PyTorch Geometric.\n",
        "\n",
        "\tAttributes:\n",
        "\t\tlin_dict (torch.nn.ModuleDict): A dictionary of linear layers for each node type.\n",
        "\t\tconvs (torch.nn.ModuleList): A list of HGTConv layers for message passing.\n",
        "\t\tlin (torch.nn.Linear): A linear layer for final node classification.\n",
        "\n",
        "\tMethods:\n",
        "\t\tforward(x_dict: Dict[str, torch.Tensor], edge_index_dict: Dict[Tuple[str, str, str], torch.Tensor]) -> torch.Tensor:\n",
        "\t\t\tDefines the forward pass of the HGT model.\n",
        "\t\"\"\"\n",
        "\tdef __init__(\n",
        "\t\t\tself,\n",
        "\t\t\thidden_channels: int,\n",
        "\t\t\tout_channels: int,\n",
        "\t\t\tnum_heads: int,\n",
        "\t\t\tnum_layers: int) -> None:\n",
        "\t\t\"\"\"\n",
        "\t\tInitializes the HGT model.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\thidden_channels (int): Number of hidden units in each layer.\n",
        "\t\t\tout_channels (int): Number of output features per node.\n",
        "\t\t\tnum_heads (int): Number of attention heads in each HGTConv layer.\n",
        "\t\t\tnum_layers (int): Number of HGTConv layers in the model.\n",
        "\t\t\"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tself.lin_dict = torch.nn.ModuleDict()\n",
        "\t\tfor node_type in data.node_types:\n",
        "\t\t\tself.lin_dict[node_type] = Linear(\n",
        "\t\t\t\tin_channels=-1,\n",
        "\t\t\t\tout_channels=hidden_channels)\n",
        "\n",
        "\t\tself.convs = torch.nn.ModuleList()\n",
        "\t\tfor _ in range(num_layers):\n",
        "\t\t\tconv = HGTConv(\n",
        "\t\t\t\tin_channels=hidden_channels,\n",
        "\t\t\t\tout_channels=hidden_channels,\n",
        "\t\t\t\tmetadata=data.metadata(),\n",
        "\t\t\t\theads=num_heads)\n",
        "\t\t\tself.convs.append(conv)\n",
        "\n",
        "\t\tself.lin = Linear(\n",
        "\t\t\tin_channels=hidden_channels,\n",
        "\t\t\tout_channels=out_channels)\n",
        "\n",
        "\tdef forward(\n",
        "\t\t\tself,\n",
        "\t\t\tx_dict: Dict[str, torch.Tensor],\n",
        "\t\t\tedge_index_dict: Dict[Tuple[str, str, str], torch.Tensor]) -> torch.Tensor:\n",
        "\t\t\"\"\"\n",
        "\t\tDefines the forward pass of the HGT model.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\tx_dict (Dict[str, torch.Tensor]): A dictionary containing node feature matrices for each node type.\n",
        "\t\t\tedge_index_dict (Dict[Tuple[str, str, str], torch.Tensor]): A dictionary containing edge indices for each edge type.\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\ttorch.Tensor: Output node embeddings for the 'author' node type.\n",
        "\t\t\"\"\"\n",
        "\t\tfor node_type, x in x_dict.items():\n",
        "\t\t\tx_dict[node_type] = self.lin_dict[node_type](x).relu_()\n",
        "\n",
        "\t\tfor conv in self.convs:\n",
        "\t\t\tx_dict = conv(x_dict, edge_index_dict)\n",
        "\n",
        "\t\treturn self.lin(x_dict['author'])\n",
        "\n",
        "hgt_model = HGT(\n",
        "    hidden_channels=64,\n",
        "    out_channels=num_classes,\n",
        "    num_heads=2,\n",
        "    num_layers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtRIvct_irxA"
      },
      "outputs": [],
      "source": [
        "# ----- Initialisation ----- #\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hgt_model = hgt_model.to(device)\n",
        "\n",
        "init_params(model=hgt_model, loader=train_loader, device=device)\n",
        "optimizer = torch.optim.Adam(hgt_model.parameters(), lr=0.01)\n",
        "print(f\"Test: {test(hgt_model, test_loader, device): 0.4f} \\n\")\n",
        "\n",
        "# ----- Training Loop ----- #\n",
        "for epoch in range(1, 51):\n",
        "    loss = train(hgt_model, train_loader, optimizer, device)\n",
        "    val_acc = test(hgt_model, val_loader, device)\n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}\")\n",
        "\n",
        "print(f\"\\n Test: {test(hgt_model, test_loader, device): 0.4f} \\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "win_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
